{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to portfolio optimization\n",
    "## Example: momentum strategy\n",
    "\n",
    "---\n",
    " \n",
    " \n",
    "- Copyright (c) Lukas Gonon, 2024. All rights reserved\n",
    "\n",
    "- Author: Lukas Gonon <l.gonon@imperial.ac.uk>\n",
    "\n",
    "- Platform: Tested on Windows 10 with Python 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: this notebook does not consitute any kind of investment advice. It merely serves as an illustrative example.\n",
    "\n",
    "We start by importing relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yahoo_fin.stock_info as yf  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct list of tickers and download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here we download the data from yahoo finance. Alternatively, see below, we can load it directly from a csv file. \n",
    "\n",
    "# Download the data \n",
    "# Specify the data range in American format\n",
    "\n",
    "start_date = \"01/01/2011\"\n",
    "end_date = \"12/31/2023\"\n",
    "tickers1 = [\"AMD\",\"AMZN\",\"AXP\",\"AZO\",\"CLX\",\"DLTR\", \"DVN\", \"FCX\", \"GOOG\", \"JPM\", \"KO\", \"MMM\",\"MSFT\",\"NFLX\",\"MRO\"]\n",
    "\n",
    "tickers2 = ['MMM', 'ABT', 'ABBV', 'ACN', 'AYI', 'ADBE', 'AMD', 'AAP', 'AES', 'AET', 'AMG', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE', 'ALXN', 'ALGN', 'ALLE', 'AGN', 'ADS', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'APC', 'ADI', 'ANDV', 'ANSS', 'ANTM', 'AON', 'AOS', 'APA', 'AIV', 'AAPL', 'AMAT', 'APTV', 'ADM', 'ARNC', 'AJG', 'AIZ', 'T', 'ADSK', 'ADP', 'AZO', 'AVB', 'AVY', 'BHGE', 'BLL', 'BAC', 'BK', 'BAX', 'BBT', 'BDX', 'BRK.B', 'BBY', 'BIIB', 'BLK', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BHF', 'BMY', 'AVGO', 'BF.B', 'CHRW', 'CA', 'COG', 'CDNS', 'CPB', 'COF', 'CAH', 'CBOE', 'KMX', 'CCL', 'CAT', 'CBG', 'CBS', 'CELG', 'CNC', 'CNP', 'CTL', 'CERN', 'CF', 'SCHW', 'CHTR', 'CHK', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'XEC', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CTXS', 'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'CXO', 'COP', 'ED', 'STZ', 'COO', 'GLW', 'COST', 'COTY', 'CCI', 'CSRA', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DAL', 'XRAY', 'DVN', 'DLR', 'DFS', 'DISCA', 'DISCK', 'DISH', 'DG', 'DLTR', 'D', 'DOV', 'DWDP', 'DPS', 'DTE', 'DRE', 'DUK', 'DXC', 'ETFC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EVHC', 'EOG', 'EQT', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ES', 'RE', 'EXC', 'EXPE', 'EXPD', 'ESRX', 'EXR', 'XOM', 'FFIV', 'FB', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC', 'FL', 'F', 'FTV', 'FBHS', 'BEN', 'FCX', 'GPS', 'GRMN', 'IT', 'GD', 'GE', 'GGP', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GS', 'GT', 'GWW', 'HAL', 'HBI', 'HOG', 'HRS', 'HIG', 'HAS', 'HCA', 'HCP', 'HP', 'HSIC', 'HSY', 'HES', 'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HPQ', 'HUM', 'HBAN', 'HII', 'IDXX', 'INFO', 'ITW', 'ILMN', 'IR', 'INTC', 'ICE', 'IBM', 'INCY', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IQV', 'IRM', 'JEC', 'JBHT', 'SJM', 'JNJ', 'JCI', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KHC', 'KR', 'LB', 'LLL', 'LH', 'LRCX', 'LEG', 'LEN', 'LUK', 'LLY', 'LNC', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'MAC', 'M', 'MRO', 'MPC', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MAT', 'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'MET', 'MTD', 'MGM', 'KORS', 'MCHP', 'MU', 'MSFT', 'MAA', 'MHK', 'TAP', 'MDLZ', 'MON', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MYL', 'NDAQ', 'NOV', 'NAVI', 'NTAP', 'NFLX', 'NWL', 'NFX', 'NEM', 'NWSA', 'NWS', 'NEE', 'NLSN', 'NKE', 'NI', 'NBL', 'JWN', 'NSC', 'NTRS', 'NOC', 'NCLH', 'NRG', 'NUE', 'NVDA', 'ORLY', 'OXY', 'OMC', 'OKE', 'ORCL', 'PCAR', 'PKG', 'PH', 'PDCO', 'PAYX', 'PYPL', 'PNR', 'PBCT', 'PEP', 'PKI', 'PRGO', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'RL', 'PPG', 'PPL', 'PX', 'PCLN', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RRC', 'RJF', 'RTN', 'O', 'RHT', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RHI', 'ROK', 'COL', 'ROP', 'ROST', 'RCL', 'CRM', 'SBAC', 'SCG', 'SLB', 'SNI', 'STX', 'SEE', 'SRE', 'SHW', 'SIG', 'SPG', 'SWKS', 'SLG', 'SNA', 'SO', 'LUV', 'SPGI', 'SWK', 'SBUX', 'STT', 'SRCL', 'SYK', 'STI', 'SYMC', 'SYF', 'SNPS', 'SYY', 'TROW', 'TPR', 'TGT', 'TEL', 'FTI', 'TXN', 'TXT', 'TMO', 'TIF', 'TWX', 'TJX', 'TMK', 'TSS', 'TSCO', 'TDG', 'TRV', 'TRIP', 'FOXA', 'FOX', 'TSN', 'UDR', 'ULTA', 'USB', 'UAA', 'UA', 'UNP', 'UAL', 'UNH', 'UPS', 'URI', 'UTX', 'UHS', 'UNM', 'VFC', 'VLO', 'VAR', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VIAB', 'V', 'VNO', 'VMC', 'WMT', 'WBA', 'DIS', 'WM', 'WAT', 'WEC', 'WFC', 'HCN', 'WDC', 'WU', 'WRK', 'WY', 'WHR', 'WMB', 'WLTW', 'WYN', 'WYNN', 'XEL', 'XRX', 'XLNX', 'XL', 'XYL', 'YUM', 'ZBH', 'ZION', 'ZTS']\n",
    "\n",
    "\n",
    "nbTickers = len(tickers2)\n",
    "nbExtract = 100 ## Number of tickers to consider\n",
    "listExtractTickers = tickers1\n",
    "listTemp = random.sample(tickers2, nbExtract)\n",
    "for i in range(nbExtract):\n",
    "    listExtractTickers.append(listTemp[i]) ## select nbExtract out of the whole list\n",
    "print(\"List of extracted tickers: \", listExtractTickers)\n",
    "\n",
    "raw_tickers = listExtractTickers\n",
    "\n",
    "## Use the above code to for a random subset of tickers. To use all of them: use the below one.\n",
    "\n",
    "raw_tickers = tickers2\n",
    "\n",
    "# Remove duplicates\n",
    "\n",
    "raw_tickers = list(dict.fromkeys(raw_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get all tickers from S&P 500.\n",
    "\n",
    "raw_tickers2 = pd.read_html(\n",
    "    'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "raw_tickers2 = list(raw_tickers2['Symbol'])\n",
    "\n",
    "## Complement the two lists.\n",
    "\n",
    "in_first = set(raw_tickers)\n",
    "in_second = set(raw_tickers2)\n",
    "\n",
    "in_second_but_not_in_first = in_second - in_first\n",
    "\n",
    "raw_tickers = raw_tickers + list(in_second_but_not_in_first)\n",
    "\n",
    "len(raw_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_second_but_not_in_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if the data is available for the given ticker for the entire time horizon. Otherwise remove ticker from the list.\n",
    "\n",
    "df = pd.DataFrame()#yf.get_data(tickers[0], start_date=start_date, end_date=end_date)['adjclose'])\n",
    "tickers = []\n",
    "for ticker in raw_tickers:\n",
    "    print(ticker)\n",
    "    try: \n",
    "        df0 = yf.get_data(ticker, start_date=start_date, end_date=end_date)['adjclose']\n",
    "        #df0.rename(ticker)\n",
    "        #print(df0.columns)\n",
    "        df = pd.concat([df, df0], axis=1)\n",
    "        tickers.append(ticker)\n",
    "    except:\n",
    "        raw_tickers.remove(ticker)\n",
    "df.columns = tickers\n",
    "print(len(tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.dropna('columns')\n",
    "\n",
    "# Print the first few rows of the data as well as information \n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingtickers=list(df.columns[df.isna().any()])\n",
    "tickers = [ticker for ticker in tickers if ticker not in missingtickers]\n",
    "print(missingtickers)\n",
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted closing price for a given ticker and the chosen dates\n",
    "get_px = lambda x: yf.get_data(x, start_date=start_date, end_date=end_date)['close']#['adjclose']\n",
    "\n",
    "# Download the data for the tickers in the list\n",
    "data = pd.DataFrame({sym:get_px(sym) for sym in tickers})\n",
    "data = data.dropna('columns')\n",
    "\n",
    "# Print the first few rows of the data as well as information \n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get closing prices of downloaded data in array\n",
    "prices = data[tickers]\n",
    "plot_ix=15\n",
    "prices.info()\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively, we can directly load the data from a csv file using this command:\n",
    "\n",
    "#prices = pd.read_csv('sp500stocks.csv',index_col=0)\n",
    "#prices.info()\n",
    "#prices.head()\n",
    "\n",
    "# This command could be used to save the data as a csv. \n",
    "#prices.to_csv('sp500stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot prices for the first few tickers\n",
    "plot_prices = (prices/np.array(prices)[0,:])[tickers[:plot_ix]]\n",
    "plot_prices.plot(figsize = (15, 6),title='Prices of' + str(plot_ix) + 'S&P500 stocks',ylabel='Adjusted closing price');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ret = np.log(prices/prices.shift(1)).dropna()*252\n",
    "log_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-sectional momentum strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a cross-sectional momentum strategy:\n",
    "\n",
    "- Sort stocks by their performance in lookback window.\n",
    "- “Buy winner and sell losers”.\n",
    "- Simplest implementation: equal weighted portfolio of 5% of most successful stocks, financed by shorting equal weighted portfolio of worst performers.\n",
    "- Long short portoflio hedges out market risk.\n",
    "- Averaging over several stocks leads to more stable performance.\n",
    "- See Jegadeesh and Titman (1993) for more details and a comprehensive study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose how many stocks to short and long at each rebalancing step\n",
    "factors = log_ret\n",
    "n_titles = int(0.05*len(tickers))\n",
    "print(n_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements the cross-sectional momentum strategy. It works as follows: we start with fixed initial capital. Then we compute the moving averages of all factors (in this case log-returns) over a pre-specified horizon. We rebalance at a given frequency (say every 20 trading dates). At each of the rebalancing dates, first we clear our position (sell all the stocks we hold, buy back those we borrowed). This leaves us with a certain amount of money (total_cap).  Then we look at the moving averages of past factors for each stock. Then we choose those stocks that performed best on average (i.e. with the highest moving average) and we buy these n_titles stocks. Similarly we short the n_titles stocks with the worst performance in the recent past (lowest moving average).  The positions we take in these stocks are equally weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_cross_sec_momentum(horizon,frequency,shift=0):\n",
    "    total_cap = 1000000.        # Initial capital\n",
    "    value = [total_cap]     # Keep track of the value of your portfolio\n",
    "    moving_averages = factors.rolling(horizon).mean().dropna()   # Construct moving averages with window size = horizon\n",
    "    n_dates = int(factors.shape[0]/frequency)  # Compute number of dates for rebalancing\n",
    "    dates = factors.index.values      # Get dates from dataframe\n",
    "    rebalancing_dates = [dates[shift+(i+1)*frequency] for i in range(n_dates)]  # Compute dates at which portfolio is rebalanced\n",
    "    units_old = np.zeros([len(tickers)])   # Initialize \"old units\" to 0\n",
    "    set_cap = False   # Set to False at the beginning\n",
    "    for i in range(len(rebalancing_dates)):\n",
    "        ## We don't trade, if we don't have enough data about past performance available yet\n",
    "        if (i+1)*frequency<horizon:\n",
    "            continue\n",
    "        date = rebalancing_dates[i] # Current date\n",
    "        rel_ma = moving_averages.loc[date,tickers]  #Access current moving averages\n",
    "        prices_date = prices.loc[date] #Access current prices\n",
    "        sort_ind = np.argsort(rel_ma) # Sort stocks according to past performance\n",
    "        long_ind = sort_ind[len(tickers)-n_titles:] # Indices of those stocks that performed best\n",
    "        short_ind = sort_ind[:n_titles] # Indices of those stocks that performed worst\n",
    "        units = np.zeros_like(rel_ma) # Initialize units to 0\n",
    "        ## In the first iteration this will be false; we start with initial capital. Afterwards this is how much our portfolio is still worth\n",
    "        if set_cap is True:\n",
    "            total_cap = value[-1]+np.sum(units_old*prices_date) # Previous value + gains you make from selling stocks (or buying back) you bought (or sold) in previous period at current price\n",
    "        units[long_ind] = total_cap/(2*n_titles) # Set equal weights for stocks that you buy\n",
    "        units[short_ind] = -total_cap/(2*n_titles) # Set equal weights for stocks that you shortsell\n",
    "        units = units/prices_date # Convert from proportion of wealth to actual units\n",
    "        ## Update value: liquidate previous position, build current one. \n",
    "        value.append(value[-1]+(np.sum(units_old*prices_date)-np.sum(units*prices_date)))\n",
    "        ## Set variables for next iteration\n",
    "        units_old = units\n",
    "        set_cap = True\n",
    "    ## At terminal time we liquidate the full position:\n",
    "    value.append(value[-1]+(np.sum(units*prices.loc[dates[-1]])))\n",
    "    \n",
    "    ## For benchmarking we may want to create some plots / get some output\n",
    "    #plt.plot(value,label=str(horizon))\n",
    "    #print(rebalancing_dates[0])\n",
    "    #print(rebalancing_dates[-3:])\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we measure the performance? A commonly used measure is the Sharpe ratio. This ratio compares the expected return from a portfolio to its risk. The Sharpe ratio is defined as \n",
    "$$ \\frac{E[R]-R_f}{Std(R)} $$\n",
    "where $E[R]$ is the expected return of the portfolio, $R_f$ is the risk-free rate, and $Std(R)$ is the standard deviation of returns.\n",
    "Let us compute the Sharpe ratio of our strategy for a choice of rebalancing frequency and time-window of moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = 50\n",
    "horizon = 100\n",
    "values = np.array(value_cross_sec_momentum(horizon,frequency))\n",
    "returns_strat = (values[1:] - values[:-1])/values[:-1]\n",
    "mu_hat = np.mean(returns_strat)\n",
    "sigma_hat = np.std(returns_strat)\n",
    "## returns and std are estimated based on a frequency 20/252 -> annualized sharpe ratio has factor np.sqrt(252/20)\n",
    "print(mu_hat/sigma_hat*np.sqrt(252/frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the performance for varying lookback periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = 20 ## rebalance every 50 trading days\n",
    "for horizon in [100+i*10 for i in range(8)]:\n",
    "    print(horizon)\n",
    "    values = np.array(value_cross_sec_momentum(horizon,frequency))\n",
    "    returns_strat = (values[1:] - values[:-1])/values[:-1]\n",
    "    mu_hat = np.mean(returns_strat)\n",
    "    sigma_hat = np.std(returns_strat)\n",
    "    ##  annualized sharpe ratio has factor np.sqrt(252(frequency))\n",
    "    print(mu_hat/sigma_hat*np.sqrt(252/frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a fixed lookback period, but shift the day on which we start the strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon=130\n",
    "sharpe=[]\n",
    "for shift in range(15):\n",
    "    #total_cap = 1000000.\n",
    "    values = np.array(value_cross_sec_momentum(horizon,50,shift))\n",
    "    returns_strat = (values[1:] - values[:-1])/values[:-1]\n",
    "    mu_hat = np.mean(returns_strat)\n",
    "    sigma_hat = np.std(returns_strat)\n",
    "    ## returns and std are estimated based on a frequency 20/252 -> annualized sharpe ratio has factor np.sqrt(252/20)\n",
    "    sharpe.append(mu_hat/sigma_hat*np.sqrt(252/frequency))\n",
    "    #print(mu_hat/sigma_hat)\n",
    "    #print(values[-1])\n",
    "    #plt.legend()\n",
    "print(np.mean(sharpe))\n",
    "print(sharpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the performance of this strategy seems to heavily depend on the day we started! Starting one day later affects also all subsequent rebalancing dates. Thus, the different shifts lead to quite a big variation of the final portfolio performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further questions: \n",
    "-  How sensitive is this to the choice of the 5% cutoff and the\n",
    "lookback window?\n",
    "- How much are the returns reduced due to transaction costs of\n",
    "1%, 0.5%, or 0.1%?\n",
    "- Can the strategy be improved by considering combinations of\n",
    "multiple lookback horizons?\n",
    "- Or does this just lead to datamining and bad out of sample\n",
    "performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-series momentum strategy based on predicted returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we aim to examine a second type of strategies based on predicted returns. We follow the same approach as above. But instead of moving averages we now use a prediction model that, based on past and current data, predicts the average returns over the next 15 days. Then again we rank the stocks according to this indicator. We buy those that our model predicts to perform best and sell those stocks that the model predicts to perform worst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by a function that creates lagged time series from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_data_averages(data, window):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data)-windowsize):\n",
    "        feature = data[i:i+windowsize,:]\n",
    "        target = np.mean(data[i+windowsize:min(i+windowsize+15,len(data)),:len(tickers)],axis=0)\n",
    "        x.append(feature)\n",
    "        y.append(target)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowsize = 50\n",
    "num_train_samples = int(log_ret.shape[0]*0.65)\n",
    "x_train, y_train = create_lagged_data_averages(np.array(log_ret)[:num_train_samples,:], windowsize)\n",
    "x_test, y_test = create_lagged_data_averages(np.array(log_ret)[num_train_samples:,:], windowsize)\n",
    "print('Shapes of training features and targets:', x_train.shape, y_train.shape)\n",
    "print('Shapes of test features and targets:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we adapt the strategy above to this more general \"predictor\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dates = len(factors.index.values)\n",
    "\n",
    "def value_strategy_predictor(horizon,frequency,predictor,shift=0,end_ind=n_dates-1):\n",
    "    total_cap = 1000000.        # Initial capital\n",
    "    value = [total_cap]     # Keep track of the value of your portfolio\n",
    "    n_dates = int(factors.shape[0]/frequency) # compute number of dates for rebalancing\n",
    "    dates = factors.index.values # Get dates from dataframe\n",
    "    ## Compute dates at which portfolio is rebalanced. We now also allow for an earlier end.\n",
    "    rebalancing_dates = [dates[min(shift+(i+1)*frequency,end_ind)] for i in range(n_dates)]\n",
    "    rebalancing_dates = list(dict.fromkeys(rebalancing_dates)) # Remove any duplicates from the list of rebalancing dates\n",
    "    units_old = np.zeros([len(tickers)])   # Initialize \"old units\" to 0\n",
    "    set_cap = False   # Set to False at the beginning\n",
    "    for i in range(len(rebalancing_dates)):\n",
    "      ## We don't trade, if we don't have enough data about past performance available yet\n",
    "        if (i+1)*frequency<horizon:\n",
    "            continue\n",
    "        date = rebalancing_dates[i] # Current date\n",
    "        ind = int(np.where(dates==date)[0]) # Access index of current date\n",
    "        inputs = np.array(factors.loc[dates[ind-horizon+1:ind+1]]) # Access past factors in a window from \"horizon\" up to today\n",
    "        pred_date = predictor(inputs) # Make predictions about future performance based on todays input\n",
    "        prices_date = prices.loc[date] #Access current prices\n",
    "        sort_ind = np.argsort(pred_date) # Sort stocks according to predicted performance\n",
    "        long_ind = sort_ind[len(tickers)-n_titles:] # Indices of those stocks that performed best\n",
    "        short_ind = sort_ind[:n_titles] # Indices of those stocks that performed worst\n",
    "        units = np.zeros([len(tickers)]) # Initialize units to 0\n",
    "        ## In the first iteration this will be false; we start with initial capital. Afterwards this is how much our portfolio is still worth\n",
    "        if set_cap is True:\n",
    "            total_cap = value[-1]+np.sum(units_old*prices_date) # Previous value + gains you make from selling stocks (or buying back) you bought (or sold) in previous period at current price\n",
    "        units[long_ind] = total_cap/(2*n_titles) # Set equal weights for stocks that you buy\n",
    "        units[short_ind] = -total_cap/(2*n_titles) # Set equal weights for stocks that you shortsell\n",
    "        units = units/prices_date # Convert from proportion of wealth to actual units\n",
    "        ## Update value: liquidate previous position, build current one. \n",
    "        value.append(value[-1]+(np.sum(units_old*prices_date)-np.sum(units*prices_date)))\n",
    "        ## Set variables for next iteration\n",
    "        units_old = units\n",
    "        set_cap = True\n",
    "    ## At terminal time we liquidate the full position:      \n",
    "    value.append(value[-1]+(np.sum(units*prices.loc[dates[end_ind]])))\n",
    "    #plt.plot(value,label=str(horizon))\n",
    "    return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us build the predictor. The features now consist of past average returns for each of the stocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mean = np.mean(np.array(x_train),axis=1)\n",
    "reg_mean = LinearRegression(fit_intercept=True).fit(x_train_mean, y_train)\n",
    "\n",
    "print(reg_mean.score(x_train_mean, y_train))\n",
    "print(sklearn.metrics.mean_squared_error(y_train,reg_mean.predict(x_train_mean)))\n",
    "\n",
    "x_test_mean = np.mean(np.array(x_test),axis=1)\n",
    "print(reg_mean.score(x_test_mean, y_test))\n",
    "print(sklearn.metrics.mean_squared_error(y_test,reg_mean.predict(x_test_mean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_predictor_mean(inputs):\n",
    "    return reg_mean.predict(np.mean(inputs,axis=0,keepdims=True))[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe=[]\n",
    "out_of_sample_ind = y_train.shape[0]\n",
    "for shift in range(9):\n",
    "    values = np.array(value_strategy_predictor(windowsize,frequency,linear_predictor_mean,shift+out_of_sample_ind))\n",
    "    returns_strat = (values[1:] - values[:-1])/values[:-1]\n",
    "    mu_hat = np.mean(returns_strat)\n",
    "    sigma_hat = np.std(returns_strat)\n",
    "    ## returns and std are estimated based on a frequency 20/252 -> annualized sharpe ratio has factor np.sqrt(252/20)\n",
    "    sharpe.append(mu_hat/sigma_hat*np.sqrt(252/frequency))\n",
    "print(np.mean(sharpe))\n",
    "print(sharpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem to work well, but again depends on the day on which we start... And again similar questions arise:\n",
    "\n",
    "-  How sensitive is this to the choice of the 5% cutoff, the\n",
    "lookback window and the rebalancing frequency?\n",
    "- How much are the returns reduced due to transaction costs of\n",
    "1%, 0.5%, or 0.1%?\n",
    "- Can the strategy be improved by considering combinations of\n",
    "multiple lookback horizons?\n",
    "- Or does this just lead to datamining and bad out of sample\n",
    "performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example: let us look at daily rebalancing instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe=[]\n",
    "out_of_sample_ind = y_train.shape[0]\n",
    "for shift in range(9):\n",
    "    values = np.array(value_strategy_predictor(windowsize,1,linear_predictor_mean,shift+out_of_sample_ind))\n",
    "    returns_strat = (values[1:] - values[:-1])/values[:-1]\n",
    "    mu_hat = np.mean(returns_strat)\n",
    "    sigma_hat = np.std(returns_strat)\n",
    "    ## returns and std are estimated based on a frequency 20/252 -> annualized sharpe ratio has factor np.sqrt(252/20)\n",
    "    sharpe.append(mu_hat/sigma_hat*np.sqrt(252/frequency))\n",
    "print(np.mean(sharpe))\n",
    "print(sharpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebalancing every second day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe=[]\n",
    "out_of_sample_ind = y_train.shape[0]\n",
    "for shift in range(9):\n",
    "    values = np.array(value_strategy_predictor(windowsize,2,linear_predictor_mean,shift+out_of_sample_ind))\n",
    "    returns_strat = (values[1:] - values[:-1])/values[:-1]\n",
    "    mu_hat = np.mean(returns_strat)\n",
    "    sigma_hat = np.std(returns_strat)\n",
    "    ## returns and std are estimated based on a frequency 20/252 -> annualized sharpe ratio has factor np.sqrt(252/20)\n",
    "    sharpe.append(mu_hat/sigma_hat*np.sqrt(252/frequency))\n",
    "print(np.mean(sharpe))\n",
    "print(sharpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further reading and more details: see, for example, the textbook \"Efficiently Inefficient\"\n",
    "    https://www.jstor.org/stable/j.ctt1287knh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fintech attempts:\n",
    "\n",
    "-  Machine learning for finding better \"predictors\"\n",
    "- Using machine learning to efficiently select portfolios in presence of high transaction costs\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6690642e02003eafad9f482803f5595e8dc0e754705b5af4548bffa979361184"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
